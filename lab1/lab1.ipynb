{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3a6647c9ee520f69ea6edb9aa8f641e3",
     "grade": false,
     "grade_id": "cell-74ae53a2a2f768b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Lab 1: Independent Component Analysis\n",
    "\n",
    "### Machine Learning 2 (2017/2018)\n",
    "\n",
    "* The lab exercises should be made in groups of two people.\n",
    "* The deadline is Thursday, April 19, 23:59.\n",
    "* Assignment should be submitted through BlackBoard! Make sure to include your and your teammates' names with the submission.\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file should be \"studentid1\\_studentid2\\_lab#\", for example, the attached file should be \"12345\\_12346\\_lab1.ipynb\". Only use underscores (\"\\_\") to connect ids, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please ask.\n",
    "* Use __one cell__ for code and markdown answers only!\n",
    "    * Put all code in the cell with the ```# YOUR CODE HERE``` comment and overwrite the ```raise NotImplementedError()``` line.\n",
    "    * For theoretical questions, put your solution using LaTeX style formatting in the YOUR ANSWER HERE cell.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* Large parts of you notebook will be graded automatically. Therefore it is important that your notebook can be run completely without errors and within a reasonable time limit. To test your notebook before submission, select Kernel -> Restart \\& Run All."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c1e4b25e4b457a2af687799f5941485c",
     "grade": false,
     "grade_id": "cell-2c0707361601df18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Literature\n",
    "In this assignment, we will implement the Independent Component Analysis algorithm as described in chapter 34 of David MacKay's book \"Information Theory, Inference, and Learning Algorithms\", which is freely available here:\n",
    "http://www.inference.phy.cam.ac.uk/mackay/itila/book.html\n",
    "\n",
    "Read the ICA chapter carefuly before you continue!\n",
    "\n",
    "### Notation\n",
    "\n",
    "$\\mathbf{X}$ is the $M \\times T$ data matrix, containing $M$ measurements at $T$ time steps.\n",
    "\n",
    "$\\mathbf{S}$ is the $S \\times T$ source matrix, containing $S$ source signal values at $T$ time steps. We will assume $S = M$.\n",
    "\n",
    "$\\mathbf{A}$ is the mixing matrix. We have $\\mathbf{X} = \\mathbf{A S}$.\n",
    "\n",
    "$\\mathbf{W}$ is the matrix we aim to learn. It is the inverse of $\\mathbf{A}$, up to indeterminacies (scaling and permutation of sources).\n",
    "\n",
    "$\\phi$ is an elementwise non-linearity or activation function, typically applied to elements of $\\mathbf{W X}$.\n",
    "\n",
    "### Code\n",
    "In the following assignments, you can make use of the signal generators listed below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "09c7db05973b399feae39aada1e549a7",
     "grade": false,
     "grade_id": "cell-3b1901f3dd2b7a59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import sys\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed\"\n",
    "\n",
    "# Signal generators\n",
    "def sawtooth(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return (((x / period - phase - 0.5) % 1) - 0.5) * 2 * amp\n",
    "\n",
    "def sine_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return np.sin((x / period - phase) * 2 * np.pi) * amp\n",
    "\n",
    "def square_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return ((np.floor(2 * x / period - 2 * phase - 1) % 2 == 0).astype(float) - 0.5) * 2 * amp\n",
    "\n",
    "def triangle_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return (sawtooth(x, period, 1., phase) * square_wave(x, period, 1., phase) + 0.5) * 2 * amp\n",
    "\n",
    "def random_nonsingular_matrix(d=2):\n",
    "    \"\"\"\n",
    "    Generates a random nonsingular (invertible) matrix of shape d*d\n",
    "    \"\"\"\n",
    "    epsilon = 0.1\n",
    "    A = np.random.rand(d, d)\n",
    "    while abs(np.linalg.det(A)) < epsilon:\n",
    "        A = np.random.rand(d, d)\n",
    "    return A\n",
    "\n",
    "def plot_signals(X, title=\"Signals\"):\n",
    "    \"\"\"\n",
    "    Plot the signals contained in the rows of X.\n",
    "    \"\"\"\n",
    "    figure()\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        ax = plt.subplot(X.shape[0], 1, i + 1)\n",
    "        plot(X[i, :])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Signal generators\n",
    "def sawtooth(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return (((x / period - phase - 0.5) % 1) - 0.5) * 2 * amp\n",
    "\n",
    "def sine_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return np.sin((x / period - phase) * 2 * np.pi) * amp\n",
    "\n",
    "def square_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return ((np.floor(2 * x / period - 2 * phase - 1) % 2 == 0).astype(float) - 0.5) * 2 * amp\n",
    "\n",
    "def triangle_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return (sawtooth(x, period, 1., phase) * square_wave(x, period, 1., phase) + 0.5) * 2 * amp\n",
    "\n",
    "def random_nonsingular_matrix(d=2):\n",
    "    \"\"\"\n",
    "    Generates a random nonsingular (invertible) matrix of shape d*d\n",
    "    \"\"\"\n",
    "    epsilon = 0.1\n",
    "    A = np.random.rand(d, d)\n",
    "    while abs(np.linalg.det(A)) < epsilon:\n",
    "        A = np.random.rand(d, d)\n",
    "    return A\n",
    "\n",
    "def plot_signals(X, title=\"Signals\"):\n",
    "    \"\"\"\n",
    "    Plot the signals contained in the rows of X.\n",
    "    \"\"\"\n",
    "    figure()\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        ax = plt.subplot(X.shape[0], 1, i + 1)\n",
    "        plot(X[i, :])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fbf8317b87200b1d50704a4f9f83327",
     "grade": false,
     "grade_id": "cell-b88ef81e682e8c77",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following code generates some toy data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b8c81ee3f333532c3484e99047e1d752",
     "grade": false,
     "grade_id": "cell-cd375ebf3b9d2dc8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "num_sources = 5\n",
    "signal_length = 500\n",
    "t = linspace(0, 1, signal_length)\n",
    "S = np.c_[sawtooth(t), sine_wave(t, 0.3), square_wave(t, 0.4), triangle_wave(t, 0.25), np.random.randn(t.size)].T\n",
    "plot_signals(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35f74a279f9a7f85e4b2c2a3a034a782",
     "grade": false,
     "grade_id": "cell-d3224a04956018cc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Make mixtures (5 points)\n",
    "Write a function `make_mixtures(S, A)' that takes a matrix of source signals $\\mathbf{S}$ and a mixing matrix $\\mathbf{A}$, and generates mixed signals $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dd909b0db0bdf3572ce99bb0638cd80e",
     "grade": false,
     "grade_id": "q1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.1 Make mixtures\n",
    "def make_mixtures(S, A):\n",
    "    '''Creates mixtures X (MxT).\n",
    "    \n",
    "    Args:\n",
    "        S: SxT matrix containing the sources\n",
    "        A: MxS matrix containing the mapping from the sources\n",
    "           to the output\n",
    "           \n",
    "    Returns:\n",
    "        MxT matrix, the mixed signals X.\n",
    "    '''\n",
    "    return A@S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "915bfe6e9ca71b5d04d01f9fdc479fd3",
     "grade": true,
     "grade_id": "q1-test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test your function\n",
    "np.random.seed(42)\n",
    "A = random_nonsingular_matrix(d=S.shape[0])\n",
    "X = make_mixtures(S, A)\n",
    "plot_signals(X, \"Mixtures\")\n",
    "\n",
    "assert X.shape == (num_sources, signal_length), \"The shape of your mixed signals is incorrect\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e7c45d1ca515bba8fa17125fa2423899",
     "grade": false,
     "grade_id": "q2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.2 Histogram (5 points)\n",
    "Write a function `plot_histograms(X)` that takes a data-matrix $\\mathbf{X}$ and plots one histogram for each signal (row) in $\\mathbf{X}$. You can use the `np.histogram()` (followed by `plot`) or `plt.hist()` function. \n",
    "\n",
    "Plot histograms of the sources and the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a8d44876a72298fe5657facdaea4aa98",
     "grade": false,
     "grade_id": "q2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.2 Histogram\n",
    "def plot_histograms(X, fig=plt.figure()):\n",
    "    '''Plot the histograms of the measured X variables.\n",
    "    \n",
    "    Args:\n",
    "        X: (MxT) matrix containing the signals\n",
    "        fig, optional: (matplotlib.pyplot.figure) \n",
    "            Figure to plot in.\n",
    "    \n",
    "    '''\n",
    "    rows, cols = shape(X);\n",
    "#     wh = int(np.ceil(np.sqrt(rows)));\n",
    "    for i, row in enumerate(X):\n",
    "        ax = fig.add_subplot(1, rows, i+1);\n",
    "        plt.hist(row);\n",
    "        if i > 0:\n",
    "            plt.setp(ax.get_yticklabels(), visible=False);\n",
    "        ax.set_xlim([-2.5, 2.5]);\n",
    "    plt.tight_layout()\n",
    "plot_histograms(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0c99ca53117a5de6626ac78ec5191e13",
     "grade": false,
     "grade_id": "cell-d14b12856ee3c83c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Which of these distributions (sources or measurements) tend to look more like Gaussians? Can you think of an explanation for this phenomenon? Why is this important for ICA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "68363e5a9239df04a2edb134f6fef78f",
     "grade": true,
     "grade_id": "q2_md",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The Gaussian noise source is obviously expected to create a distribution which tends to look like a Gaussian. The sawtooth and the triangle are expected to look uniform. The block wave is degenerated. And the sine wave like a half-pipe. The measurements will look like a combination of these, depending on the values in A. For ICA it is important to be able to separate the sources, sinve the Gaussian is rotation invariant, ICA will not work if all sources are Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implicit priors (20 points)\n",
    "As explained in MacKay's book, an activation function $\\phi$ used in the ICA learning algorithm corresponds to a prior distribution over sources. Specifically, $\\phi(a) = \\frac{d}{da} \\ln p(a)$. For each of the following activation functions, *derive* the source distribution they correspond to.\n",
    "$$\\phi_0(a) = -\\tanh(a)$$\n",
    "$$\\phi_1(a) = -a + \\tanh(a)$$\n",
    "$$\\phi_2(a) = -a^3$$ \n",
    "$$\\phi_3(a) = -\\frac{6a}{a^2 + 5}$$\n",
    "\n",
    "Give your answer without the normalizing constant, so an answer of the form $p(a) \\propto \\verb+[answer]+$ is ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6e7718327a5140638e6973ae2a5f2c32",
     "grade": true,
     "grade_id": "q3_md",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "\\begin{align}\n",
    "    p(a) & \\propto \\frac{1}{cosh(a)} \\\\\n",
    "    p(a) & \\propto cosh(a) \\exp\\left[-\\frac{a^2}{2}\\right] \\\\\n",
    "    p(a) & \\propto \\exp\\left[-\\frac{a^4}{4}\\right] \\\\\n",
    "    p(a) & \\propto \\frac{1}{(a^2 +5)^3} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c988368caa45cdd67f4849a3600d4aca",
     "grade": false,
     "grade_id": "q3_p0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def phi_0(a):\n",
    "    return -np.tanh(a)\n",
    "\n",
    "def p_0(a):\n",
    "    return 1/np.cosh(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "550925dfeef41c7d86949b463b6449c7",
     "grade": false,
     "grade_id": "q3_p1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def phi_1(a):\n",
    "    return -a + np.tanh(a)\n",
    "\n",
    "def p_1(a):\n",
    "    return np.cosh(a) * np.exp(-0.5*(a**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "41afbf475bf8b13a584a9e4559f1b5e0",
     "grade": false,
     "grade_id": "q3_p2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def phi_2(a):\n",
    "    return -a**3\n",
    "\n",
    "def p_2(a):\n",
    "    return np.exp(-0.25*(a**4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1492e648664759bab9cf523d597ac4f4",
     "grade": false,
     "grade_id": "q3_p3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def phi_3(a):\n",
    "    return -6*a / (a**2 + 5)\n",
    "\n",
    "def p_3(a):\n",
    "    return (a**2 + 5)**(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1afb0c8d8c70d0e1c8b37ee2b849a049",
     "grade": true,
     "grade_id": "q3_p_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "activation_functions = [phi_0, phi_1, phi_2, phi_3]\n",
    "priors = [p_0, p_1, p_2, p_3]\n",
    "\n",
    "a = np.linspace(-5, 5, 1000)\n",
    "for prior in priors:\n",
    "    assert prior(a).shape == (1000, ), \"Wrong output shape\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(-5, 5, 10)\n",
    "for act_f in activation_functions:\n",
    "    print(act_f(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "32f796b616383f2d260dabae444972bc",
     "grade": false,
     "grade_id": "q3_plot_text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Plot the activation functions and the corresponding prior distributions, from $a = -5$ to $5$ (hint: use the lists defined in the cell above). Compare the shape of the priors to the histogram you plotted in the last question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "abc7a7000844035a0d733d272264faec",
     "grade": true,
     "grade_id": "q3_plots",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.3 Implicit priors (continued)\n",
    "a = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def plot_actv(a, priors, fig=plt.figure()):\n",
    "    '''Plot the activation functions with the corresponding prioirs\n",
    "    \n",
    "    Args:\n",
    "        a: (list) the activation functions\n",
    "        priors: (list of func) List with priors.\n",
    "        fig, optional: (matplotlib.pyplot.figure) \n",
    "            Figure to plot in.\n",
    "    \n",
    "    '''\n",
    "    rows = len(priors)\n",
    "#     wh = int(np.ceil(np.sqrt(rows)));\n",
    "    for i, prior in enumerate(priors):\n",
    "        ax = fig.add_subplot(1, rows, i+1);\n",
    "        ax.plot(a, prior(a))\n",
    "        if i > 0:\n",
    "            plt.setp(ax.get_yticklabels(), visible=False);\n",
    "        ax.set_xlim([-5, 5]);\n",
    "    plt.tight_layout()\n",
    "plot_actv(a, activation_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2b90e2314a2a97cba79993eef89c559",
     "grade": false,
     "grade_id": "q4_text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.4 Whitening (15 points)\n",
    "Some ICA algorithms can only learn from whitened data. Write a method `whiten(X)` that takes a $M \\times T$ data matrix $\\mathbf{X}$ (where $M$ is the dimensionality and $T$ the number of examples) and returns a whitened matrix. If you forgot what whitening is or how to compute it, various good sources are available online, such as http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf. Your function should also center the data before whitening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "97168afc19ce18ce7d48d4f0b26e93ab",
     "grade": true,
     "grade_id": "q4",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.4 Whitening\n",
    "import numpy.matlib\n",
    "def whiten(X):\n",
    "    # center matrix\n",
    "#     Xw = X\n",
    "#     for i in range(X.shape[0]):\n",
    "#         Xw[i] -= mean(Xw[i])\n",
    "    Xw = (X.T - (mean(X, 1)).T).T\n",
    "    # use SVD to perform whitening\n",
    "    U, s, Vt = np.linalg.svd(Xw, full_matrices=False)\n",
    "    return np.dot(U, Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79cbb5c6c61c68e2097837ee0f63d456",
     "grade": true,
     "grade_id": "q4_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test your function\n",
    "Xw = whiten(X)\n",
    "assert Xw.shape == (num_sources, signal_length), \"The shape of your mixed signals is incorrect\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "625949cfd87388ae6dd9a9f18a99c2a5",
     "grade": false,
     "grade_id": "cell-699b10652d80628b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.5 Interpret results of whitening (10 points)\n",
    "Make 3 figures, one for the sources, one for measurements and one for the whitened measurements. In each figure, make $5 \\times 5$ subplots with scatter plots for each pair of signals. Each axis represents a signal and each time-instance is plotted as a dot in this space. You can use the `plt.scatter()` function. Describe what you see.\n",
    "\n",
    "Now compute and visualize the covariance matrix of the sources, the measurements and the whitened measurements. You can visualize each covariance matrix using this code:\n",
    "```python\n",
    "# Dummy covariance matrix C;\n",
    "C = np.eye(5)  \n",
    "ax = imshow(C, cmap='gray', interpolation='nearest')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8ea944dad9ce3a262c9ff73cbc99042a",
     "grade": true,
     "grade_id": "q5",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.5 Interpret results of whitening\n",
    "import itertools\n",
    "def plot_signalpairs (X):\n",
    "    for i,j in itertools.product(range(0,5), repeat=2):\n",
    "        plt.subplot(5,5, i*5+j+1)\n",
    "        x = X[i]\n",
    "        y = X[j]\n",
    "        plt.scatter(x,y)\n",
    "        \n",
    "figsize = (10, 10)\n",
    "plt.figure(1, figsize=figsize)\n",
    "plot_signalpairs(S)\n",
    "\n",
    "plt.figure(2, figsize=figsize)\n",
    "plot_signalpairs(X)\n",
    "\n",
    "plt.figure(3, figsize=figsize)\n",
    "Y = whiten(X)\n",
    "plot_signalpairs(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4e234e08d4788d7a8c7bc83ae3d6ec5b",
     "grade": false,
     "grade_id": "cell-ced0d6068d7ea315",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Are the signals independent after whitening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c78b3ea401aff2cd6a1c2cf731608f57",
     "grade": true,
     "grade_id": "q5_md",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Yes, whitening first decorrelates the data using the eigenvalue decomposition (or SVD in our case). No correlation means independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "99a1cdcf7805a6b054096632a8276932",
     "grade": false,
     "grade_id": "cell-04990989c6d56676",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.6 Covariance (5 points)\n",
    "Explain what a covariant algorithm is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4212bce8f348cb27b4fc84801b1964e3",
     "grade": true,
     "grade_id": "q6",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "A covariant algorithm is a algorithm that gives the same results independent of the units in which quantities are measured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "13e8de49c4a8eb709ff6c9db6f6c1462",
     "grade": false,
     "grade_id": "cell-218d0fc3ca6d2ac5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.7 Independent Component Analysis (25 points)\n",
    "Implement the covariant ICA algorithm as described in MacKay. Write a function `ICA(X, activation_function, learning_rate)`, that returns the demixing matrix $\\mathbf{W}$. The input `activation_function` should accept a function such as `lambda a: -tanh(a)`. Update the gradient in batch mode, averaging the gradients over the whole dataset for each update. Make it efficient, so use matrix operations instead of loops where possible (loops are slow in interpreted languages such as python and matlab, whereas matrix operations are internally computed using fast C code). Experiment with the learning rate and the initialization of $\\mathbf{W}$. Your algorithm should be able to converge (i.e. `np.linalg.norm(grad) < 1e-5`) in within 10000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4911d44f8014085efd7ebf31ae2e35aa",
     "grade": false,
     "grade_id": "q7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.7 Independent Component Analysis\n",
    "def ICA(X, activation_function, learning_rate=0.08, max_iters=10000, print_every=100000):\n",
    "    '''Applies Independent Component\n",
    "    '''\n",
    "    # Initializes weights\n",
    "    num_sources, num_samples = shape(X)\n",
    "    W = random_nonsingular_matrix(num_sources)\n",
    "    \n",
    "    i, grad_norm = 1, np.inf\n",
    "    while i < max_iters and (grad_norm > 1e-5):\n",
    "        # ICA algorithm\n",
    "        A = W @ X\n",
    "        Z = activation_function(A)\n",
    "        Xn = W.T @ A\n",
    "        dW = W + (Z @ Xn.T) / num_samples\n",
    "        \n",
    "        # norm of grad\n",
    "        grad_norm = linalg.norm(dW)\n",
    "        \n",
    "        # update weights\n",
    "        W += learning_rate * dW\n",
    "        \n",
    "        if not i % print_every:\n",
    "            print('Iteration %i, Error: %.5f' % (i, grad_norm))\n",
    "        i += 1\n",
    "    if i == max_iters:\n",
    "        print('Not converged.')\n",
    "    else:\n",
    "        print('Converged in %i iterations, final weight gradient norm %.5f' % (i, grad_norm))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a9b8921e43c1fae648d7c8d16fb1b4f2",
     "grade": true,
     "grade_id": "q7_test",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We will test your function so make sure it runs with only X and phi as input, and returns only W\n",
    "# Also it should converge for all activation functions\n",
    "\n",
    "W_estimates = [ICA(Xw, activation_function=phi) for phi in activation_functions]\n",
    "assert all([W_est.shape == (num_sources, num_sources) for W_est in W_estimates])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28123038a523935dbb3f1030ed731cd0",
     "grade": false,
     "grade_id": "cell-c334b668babfc19b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.8 Experiments  (5 points)\n",
    "Run ICA on the provided signals using each activation function $\\phi_0, \\ldots, \\phi_3$ (or reuse `W_estimates`). Use the found demixing matrix $\\mathbf{W}$ to reconstruct the signals and plot the retreived signals for each choice of activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5634a228c22e83f776a6aae263f8c397",
     "grade": true,
     "grade_id": "q8",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# 1.8 Experiments\n",
    "# YOUR CODE HERE\n",
    "for W in W_estimates:\n",
    "    plot_signals(W@Xw, title=\"Reconstructed signals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc84d60e05b8e0b1fbb556452fb6c998",
     "grade": false,
     "grade_id": "q7_whitening_question",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def does_whitening_make_a_difference():\n",
    "    # Does it make a difference (in terms of speed of convergence) \n",
    "    # if you whiten your data before running ICA?\n",
    "    \n",
    "    t_start = time.time()\n",
    "    W_estimates = [ICA(Xw, activation_function=phi) for phi in activation_functions]\n",
    "    time_whitening = time.time() - t_start\n",
    "    print('Time it takes to converges with whitening: %s' % time_whitening)\n",
    "    \n",
    "    t_start = time.time()\n",
    "    W_estimates = [ICA(X, activation_function=phi) for phi in activation_functions]\n",
    "    time_no_whitening = time.time() - t_start\n",
    "    print('Time it takes to converge without whitening: %s' % time_no_whitening)\n",
    "    \n",
    "    # Return True or False\n",
    "    # YOUR CODE HERE\n",
    "    return time_whitening < time_no_whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "55b1d872565bfd67647b69995debe886",
     "grade": true,
     "grade_id": "q7_whitening_test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(does_whitening_make_a_difference()) == bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a198f770a9935d1a650e4bba2503762f",
     "grade": false,
     "grade_id": "cell-99b165e65b0a60ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.9 Audio demixing (10 points)\n",
    "The 'cocktail party effect' refers to the ability humans have to attend to one speaker in a noisy room. We will now use ICA to solve a similar but somewhat idealized version of this problem. The code below loads 5 sound files and plots them.\n",
    "\n",
    "Use a random non-singular mixing matrix to mix the 5 sound files. You can listen to the results in your browser using `play_signals`, or save them to disk if this does not work for you. Plot histograms of the mixed audio and use your ICA implementation to de-mix these and reproduce the original source signals. As in the previous exercise, try each of the activation functions.\n",
    "\n",
    "Keep in mind that this problem is easier than the real cocktail party problem, because in real life there are often more sources than measurements (we have only two ears!), and the number of sources is unknown and variable. Also, mixing is not instantaneous in real life, because the sound from one source arrives at each ear at a different point in time. If you have time left, you can think of ways to deal with these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ac441f66ac2d1be0b849897cf8678a02",
     "grade": false,
     "grade_id": "cell-0f323b63610fa06e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio, display, Markdown\n",
    "\n",
    "# Save mixtures to disk, so you can listen to them in your audio player\n",
    "def save_wav(data, out_file, rate):\n",
    "    scaled = np.int16(data / np.max(np.abs(data)) * 32767)\n",
    "    scipy.io.wavfile.write(out_file, rate, scaled)\n",
    "\n",
    "# Or play them in your browser\n",
    "def play_signals(S, sample_rate, title=\"Signals\"):\n",
    "    display(Markdown(title))\n",
    "    for signal in S:\n",
    "        display(Audio(signal, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6834a2b5abc4349903a28abaff35ab08",
     "grade": false,
     "grade_id": "cell-394dc65db5a26c2c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load audio sources\n",
    "source_files = ['beet.wav', 'beet9.wav', 'beet92.wav', 'mike.wav', 'street.wav']\n",
    "wav_data = []\n",
    "sample_rate = None\n",
    "for f in source_files:\n",
    "    sr, data = scipy.io.wavfile.read(f, mmap=False)\n",
    "    if sample_rate is None:\n",
    "        sample_rate = sr\n",
    "    else:\n",
    "        assert(sample_rate == sr)\n",
    "    wav_data.append(data[:190000])  # cut off the last part so that all signals have same length\n",
    "\n",
    "# Create source and measurement data\n",
    "S_audio = np.c_[wav_data]\n",
    "plot_signals(S_audio)\n",
    "play_signals(S_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c05f3f53c93aebda8b45a81d17ed5e01",
     "grade": true,
     "grade_id": "q9",
     "locked": false,
     "points": 6,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.9 Audio demixing\n",
    "# YOUR CODE HERE\n",
    "A_audio = random_nonsingular_matrix(d=S_audio.shape[0])\n",
    "X_audio = make_mixtures(S_audio, A_audio)\n",
    "plot_signals(X_audio)\n",
    "play_signals(X_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2signals(X1, X2, title1=\"Reconstructe signals\", title2='Signals', figsize=(10, 5)):\n",
    "    \"\"\"Plot two signals contained in the rows of X1 and X2\n",
    "    \"\"\"\n",
    "    fig = figure(figsize=figsize)\n",
    "    assert X1.shape == X2.shape\n",
    "    num_s = X1.shape[0]\n",
    "    colors = ['blue', 'red']\n",
    "    index = 0\n",
    "    for i in range(X1.shape[0]):\n",
    "        index += 1\n",
    "        ax = plt.subplot(num_s, 2, index)\n",
    "        plot(X1[i, :], color=colors[0])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if i == 0:\n",
    "            ax.set_title(title1)\n",
    "        \n",
    "        index += 1\n",
    "        ax = plt.subplot(num_s, 2, index)\n",
    "        plot(X2[i, :], color=colors[1])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if i == 0:\n",
    "            ax.set_title(title2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw_audio = whiten(X_audio)\n",
    "W_audio_est = [ICA(Xw_audio, activation_function=phi) for phi in activation_functions]\n",
    "for W in W_audio_est:\n",
    "    plot_2signals(W@Xw_audio, S_audio)\n",
    "#     play_signals(W@Xw_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best denoiing matrices were create with the first and last activation function\n",
    "W = W_audio_est[0]\n",
    "plot_2signals(W@Xw_audio, S_audio)\n",
    "play_signals(W@Xw_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W_audio_est[3]\n",
    "plot_2signals(W@Xw_audio, S_audio)\n",
    "play_signals(W@Xw_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e73ba2e2bcaacfb043748d11b67dd3ec",
     "grade": false,
     "grade_id": "q9_report_text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Report your results. Using which activation functions ICA recovers the sources?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "13100a4d8859d74dcf8e112d3e129b4f",
     "grade": true,
     "grade_id": "q9_report_answer",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "With activation functions $\\phi_0(a) = -\\tanh(a)$ and $\\phi_3(a) = -\\frac{6a}{a^2 + 5}$ the best demixing matrices were found. It is expected that $\\phi_2(a) = -a^3$ is not a good choice for demixing, because it implies a rotation invariant prior $p_2(a) \\propto \\exp[-\\frac{a^4}{4}]$ (a Gauss with a faster decay), as explained in chapter 34 of David MacKay's book \"Information Theory, Inference, and Learning Algorithms\". The other actication function ($\\phi_1(a) = -a + \\tanh(a)$) has a linear term, which implies a gaussian prior. (The exponent in $p_1(a) \\propto cosh(a) \\exp\\left[-\\frac{a^2}{2}\\right]$.) Probably due to this term, ICP with this activation function also does not generate a good demixing matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "63f9a427a789d0427bbb67aa7fea2c96",
     "grade": false,
     "grade_id": "cell-c6d32c3d2df970f1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.10 Excess Kurtosis (15 points)\n",
    "The (excess) kurtosis is a measure of 'peakedness' of a distribution. It is defined as\n",
    "$$\n",
    "\\verb+Kurt+[X] = \\frac{\\mu_4}{\\sigma^4} - 3 = \\frac{\\operatorname{E}[(X-{\\mu})^4]}{(\\operatorname{E}[(X-{\\mu})^2])^2} - 3\n",
    "$$\n",
    "Here, $\\mu_4$ is known as the fourth moment about the mean, and $\\sigma$ is the standard deviation.\n",
    "The '-3' term is introduced so that a Gaussian random variable has 0 excess kurtosis.\n",
    "We will now try to understand the performance of the various activation functions by considering the kurtosis of the corresponding priors, and comparing those to the empirical kurtosis of our data.\n",
    "\n",
    "#### 1.10.1 (10 points)\n",
    "First, compute analytically the kurtosis of the four priors that you derived from the activation functions before. You may find it helpful to use an online service such as [Wolfram Alpha](https://www.wolframalpha.com/) or [Integral Calculator](https://www.integral-calculator.com/) to (help you) evaluate the required integrals. Give your answer as both an exact expression as well as a numerical approximation (for example $\\frac{\\pi}{2} \\approx 1.571$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b228a02df9a733ec2591fcd236f1ee1",
     "grade": true,
     "grade_id": "q10_1",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f6e3f3dc5475c4e1f9166baad21f50b0",
     "grade": false,
     "grade_id": "q10_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### Include your answer here (you can use math.gamma if needed)\n",
    "def get_kurtosis():\n",
    "    # Return a list with 4 numbers / expressions\n",
    "    # return [0, 0, 0, 0]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "470ef529b6f5c2b63354dc6e2dcfa12c",
     "grade": true,
     "grade_id": "q10_test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's check\n",
    "kurtosis = get_kurtosis()\n",
    "print (kurtosis)\n",
    "assert len(kurtosis) == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "81adb9331459b7ad5b4715db50bd818c",
     "grade": false,
     "grade_id": "cell-dfc3f096ad8ab2fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.10.2 (5 points)\n",
    "Now use the `scipy.stats.kurtosis` function, with the `fisher` option set to `True`, to compute the empirical kurtosis of the dummy signals and the real audio signals.\n",
    "\n",
    "Can you use this data to explain the performance of the various activation functions on the synthetic and real data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "abb69b34260231dc721c05556aae55f4",
     "grade": false,
     "grade_id": "q10_2_code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.10.2 Excess Kurtosis\n",
    "# YOUR CODE HERE\n",
    "from scipy.stats import kurtosis\n",
    "for s in S:\n",
    "    print(kurtosis(s, fisher=True))\n",
    "plot_signals(S)\n",
    "for s in S_audio:\n",
    "    print(kurtosis(s, fisher=True))\n",
    "plot_signals(S_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "38d2dc1d593a547e0fab40695e595e68",
     "grade": true,
     "grade_id": "q10_2_md",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
